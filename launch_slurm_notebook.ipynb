{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "30d80742",
      "metadata": {
        "id": "30d80742",
        "outputId": "a6d68188-dbc1-46f7-fef6-e14b8b9ae09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Search Stats:\n",
            "\n",
            "  - md: 50 configurations\n",
            "\n",
            "Total configurations: 50\n",
            "Press Enter to continue... or Ctrl+C to exit.\n",
            "Running command directly (not on SLURM): python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid data.data_dir=$TMP_SHARED/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.01 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "Running command directly (not on SLURM): python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid data.data_dir=$TMP_SHARED/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.01 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "Running command directly (not on SLURM): python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid data.data_dir=$TMP_SHARED/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.25 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "Running command directly (not on SLURM): python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid data.data_dir=$TMP_SHARED/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.25 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "Running command directly (not on SLURM): python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid data.data_dir=$TMP_SHARED/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.5 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3482110187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m         }\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Launch the job with the hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3482110187.py\u001b[0m in \u001b[0;36mlaunch_job\u001b[0;34m(**hp)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If not using SLURM, just run the command directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running command directly (not on SLURM):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_grid_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "from textwrap import dedent\n",
        "from itertools import product\n",
        "import os\n",
        "CONDA_ENV_NAME = \"HL-env\"\n",
        "REPO_DIR = os.path.abspath(\".\")  # adjust if needed\n",
        "SWEEP_CONFIG = \"grid\"\n",
        "PROJECT = f\"decouple-alphas-{SWEEP_CONFIG}\"\n",
        "data = True # add the data param?\n",
        "slurm = False  # whether to launch the jobs on SLURM or not\n",
        "\n",
        "\n",
        "# Parameters that represent each unique optimisation space\n",
        "# You can also make an item a lambda function to evaluate it dynamically where the argument is the hyperparameter dictionary\n",
        "grid = {\n",
        "    \"default\": {\n",
        "        \"model\": [\"basic_mlp\"],\n",
        "        \"task\": [\"fmnist\"],\n",
        "        \"optimizer.lr\": [0.1, 0.5],\n",
        "        # \"optimizer.weight_decay\": [0.0, 1e-4, 1e-3],\n",
        "        # \"optimizer.momentum\": [0.0, 0.9, 0.99],\n",
        "        \"corruption.alpha\": [0.01, 0.25, 0.5, 0.75, 0.99],\n",
        "        \"trainer.min_epochs\": [\"20\"],\n",
        "        \"trainer.max_epochs\": [\"20\"],\n",
        "    },\n",
        "    \"md\": {\n",
        "        \"optimizer.update_alg\": ['md'],\n",
        "        \"optimizer.alpha\": [0.01, 0.25, 0.5, 0.75, 0.99],\n",
        "        \"optimizer.block_size\": ['4'],\n",
        "    },\n",
        "    # \"gd\": {\n",
        "    #     \"optimizer.update_alg\": ['gd'],\n",
        "    # },\n",
        "}\n",
        "\n",
        "def launch_job(**hp):\n",
        "    \"\"\"\n",
        "    Launch a job on SLURM with the specified parameters.\n",
        "\n",
        "    args == hyper params\n",
        "    \"\"\"\n",
        "    # if any value is a lambda function, evaluate it with the current hp\n",
        "    for key, value in hp.items():\n",
        "        if callable(value):\n",
        "            hp[key] = value(hp)\n",
        "\n",
        "    name = \"_\".join([str(hp[k]) for k in sorted(hp)])\n",
        "    study_name = f\"study_{name}\"\n",
        "    group = name\n",
        "\n",
        "    data_dir = \"$TMP_SHARED\"\n",
        "    # Create the batch script as a multi-line string\n",
        "    template_script = dedent(f\"\"\"\\\n",
        "        #!/bin/bash\n",
        "        #SBATCH --job-name={name}\n",
        "        #SBATCH --output=slurm-logs/{PROJECT}/{name}_%j.out\n",
        "        #SBATCH --error=slurm-logs/{PROJECT}/{name}_%j.err\n",
        "        #SBATCH --time=01:00:00\n",
        "        #SBATCH --partition=gpu\n",
        "        #SBATCH --gres=gpu:1\n",
        "        #SBATCH --mem=16G\n",
        "        #SBATCH --cpus-per-task=4\n",
        "\n",
        "        module load miniforge\n",
        "        conda activate $HOME/{CONDA_ENV_NAME}\n",
        "\n",
        "        export CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "\n",
        "        LOGGING=\"$SCRATCH/{PROJECT}/{study_name}\"\n",
        "\n",
        "        mkdir -p \"$LOGGING\"\n",
        "        CHKP=\"$LOGGING/last.ckpt\"\n",
        "\n",
        "        cd $LOGGING\n",
        "        echo \"Copying data from {REPO_DIR}/data into {data_dir}/data\"\n",
        "        cp -r \"{REPO_DIR}/data\" \"{data_dir}/data\"\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", f\"{REPO_DIR}/src/train.py\", \"-m\",\n",
        "        f\"hydra.sweeper.study_name={study_name}\",\n",
        "        f\"hparams_search={SWEEP_CONFIG}\",\n",
        "        f\"logger.group={group}\",\n",
        "        f\"save_dir=$LOGGING\" if slurm else f\"save_dir={REPO_DIR}/logs/{PROJECT}/{study_name}\",\n",
        "        f\"logger.project={PROJECT}\",\n",
        "    ]\n",
        "\n",
        "    if data:\n",
        "        cmd.append(f\"data.data_dir={data_dir}/data\")\n",
        "\n",
        "\n",
        "    # the keu is the name of the hyperparameter, the value is the value to set it to\n",
        "    for key, value in hp.items():\n",
        "        cmd.append(f\"{key}={value}\")\n",
        "\n",
        "    # Add the command to run the script\n",
        "    batch_script = template_script + \"\\n\" + \" \".join(cmd) + \"\\n\" + \"echo 'Job completed.'\\n\"\n",
        "\n",
        "    # Write the script to a temp file (can be named uniquely)\n",
        "    script_filename = f\"tmp.sh\"\n",
        "\n",
        "    if slurm:\n",
        "        with open(script_filename, \"w\") as f:\n",
        "            f.write(batch_script)\n",
        "\n",
        "        # Launch the job using sbatch\n",
        "        subprocess.run([\"sbatch\", script_filename])\n",
        "    else:\n",
        "        # If not using SLURM, just run the command directly\n",
        "        print(\"Running command directly (not on SLURM):\", \" \".join(cmd))\n",
        "        subprocess.run(cmd)\n",
        "\n",
        "def print_grid_stats(grid):\n",
        "    default = grid.get(\"default\", {})\n",
        "    total = 0\n",
        "\n",
        "    print(\"Grid Search Stats:\\n\")\n",
        "\n",
        "    for space, params in grid.items():\n",
        "        if space == \"default\":\n",
        "            continue\n",
        "\n",
        "        # Merge default with specific subspace params\n",
        "        full_params = {**default, **params}\n",
        "        keys = sorted(full_params.keys())\n",
        "        values_list = [full_params[key] for key in keys]\n",
        "\n",
        "        num_configs = 1\n",
        "        for v in values_list:\n",
        "            num_configs *= len(v)\n",
        "\n",
        "        print(f\"  - {space}: {num_configs} configurations\")\n",
        "        total += num_configs\n",
        "\n",
        "    print(f\"\\nTotal configurations: {total}\")\n",
        "\n",
        "\n",
        "print_grid_stats(grid)\n",
        "input(\"Press Enter to continue... or Ctrl+C to exit.\")\n",
        "for space, params in grid.items():\n",
        "    if space == \"default\":\n",
        "        continue\n",
        "\n",
        "    # Add the default parameters to the grid\n",
        "    full_params = {**grid[\"default\"], **params}\n",
        "    keys = sorted(full_params.keys())\n",
        "    values_list = [full_params[key] for key in keys]\n",
        "\n",
        "    for values in product(*values_list):\n",
        "        hp = {\n",
        "            key: value\n",
        "            for key, value in zip(keys, values)\n",
        "        }\n",
        "        # Launch the job with the hyperparameters\n",
        "        launch_job(**hp)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/clarakuempel/HeterosynapticLearning.git"
      ],
      "metadata": {
        "id": "mapNtD3bKIa9",
        "outputId": "41aa6718-fc17-4b3c-a42c-eea24ce8fed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mapNtD3bKIa9",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HeterosynapticLearning'...\n",
            "remote: Enumerating objects: 944, done.\u001b[K\n",
            "remote: Counting objects: 100% (313/313), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 944 (delta 201), reused 246 (delta 143), pack-reused 631 (from 1)\u001b[K\n",
            "Receiving objects: 100% (944/944), 3.75 MiB | 8.29 MiB/s, done.\n",
            "Resolving deltas: 100% (568/568), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "RnzKguxiKQFZ",
        "outputId": "cc5483a3-c650-4634-c87e-ee1f301247f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RnzKguxiKQFZ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout prune-tests"
      ],
      "metadata": {
        "id": "vTUoF0HHK4Av",
        "outputId": "b2572f1b-ea83-4919-893b-b435924ef777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vTUoF0HHK4Av",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/HeterosynapticLearning\n",
        "\n",
        "# check branches\n",
        "!git branch -a\n",
        "\n",
        "# switch branch\n",
        "!git checkout prune-tests\n",
        "\n",
        "# install requirements from inside repo\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "2GRmdd5jLDYa",
        "outputId": "49907cfe-f1e1-40cb-99b3-4cb9dfb97b9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2GRmdd5jLDYa",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HeterosynapticLearning\n",
            "* \u001b[32mmain\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/changes\u001b[m\n",
            "  \u001b[31mremotes/origin/instantiate\u001b[m\n",
            "  \u001b[31mremotes/origin/lr_schdl\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n",
            "  \u001b[31mremotes/origin/mdM\u001b[m\n",
            "  \u001b[31mremotes/origin/nonCausal\u001b[m\n",
            "  \u001b[31mremotes/origin/optuna-optim\u001b[m\n",
            "  \u001b[31mremotes/origin/penn-fix\u001b[m\n",
            "  \u001b[31mremotes/origin/prune-tests\u001b[m\n",
            "  \u001b[31mremotes/origin/rerun-corrup\u001b[m\n",
            "  \u001b[31mremotes/origin/test-penn-treebank-cluster\u001b[m\n",
            "Branch 'prune-tests' set up to track remote branch 'prune-tests' from 'origin'.\n",
            "Switched to a new branch 'prune-tests'\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Collecting lightning>=2.0.0 (from -r requirements.txt (line 4))\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.17.1 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.16.2, 0.17.2, 0.18.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.17.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/HeterosynapticLearning\n",
        "!sed -i '/torchtext/d' requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "UtbHQhm7LeEI",
        "outputId": "e0555f53-4cd2-4ea3-b47b-f249bfe87312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UtbHQhm7LeEI",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HeterosynapticLearning\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.23.0+cu126)\n",
            "Collecting lightning>=2.0.0 (from -r requirements.txt (line 4))\n",
            "  Using cached lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting torchdata==0.7.1 (from -r requirements.txt (line 8))\n",
            "  Downloading torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting portalocker==3.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting hydra-core==1.3.2 (from -r requirements.txt (line 12))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting hydra-colorlog==1.2.0 (from -r requirements.txt (line 13))\n",
            "  Downloading hydra_colorlog-1.2.0-py3-none-any.whl.metadata (949 bytes)\n",
            "Collecting hydra-optuna-sweeper==1.2.0 (from -r requirements.txt (line 14))\n",
            "  Downloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (0.22.2)\n",
            "Collecting rootutils (from -r requirements.txt (line 20))\n",
            "  Downloading rootutils-1.0.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 22)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (0.13.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1->-r requirements.txt (line 8)) (2.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1->-r requirements.txt (line 8)) (2.32.4)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2->-r requirements.txt (line 12)) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2->-r requirements.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core==1.3.2->-r requirements.txt (line 12)) (25.0)\n",
            "Collecting colorlog (from hydra-colorlog==1.2.0->-r requirements.txt (line 13))\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting optuna<3.0.0,>=2.10.0 (from hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading optuna-2.10.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->-r requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.15.0->-r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.15.0->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0.0->-r requirements.txt (line 4)) (6.0.3)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0.0->-r requirements.txt (line 4))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting torchmetrics<3.0,>0.7.0 (from lightning>=2.0.0->-r requirements.txt (line 4))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from lightning>=2.0.0->-r requirements.txt (line 4)) (4.67.1)\n",
            "Collecting pytorch-lightning (from lightning>=2.0.0->-r requirements.txt (line 4))\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 17)) (2.40.0)\n",
            "Requirement already satisfied: python-dotenv>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from rootutils->-r requirements.txt (line 20)) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 22)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 22)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 22)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 23)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 23)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 23)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 23)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 23)) (3.2.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (3.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17)) (4.0.12)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.12/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (1.16.5)\n",
            "Collecting cliff (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading cliff-4.11.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting cmaes>=0.8.2 (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading cmaes-0.12.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.12/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (1.16.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (2.0.43)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 17)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 22)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1->-r requirements.txt (line 8)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata==0.7.1->-r requirements.txt (line 8)) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning>=2.0.0->-r requirements.txt (line 4)) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 17)) (5.0.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (3.2.4)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (1.3.10)\n",
            "Collecting autopage>=0.4.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading autopage-0.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting cmd2>=1.0.0 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading cmd2-2.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (3.16.0)\n",
            "Collecting stevedore>=2.0.1 (from cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: pyperclip>=1.8 in /usr/local/lib/python3.12/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (1.11.0)\n",
            "Collecting rich-argparse>=1.7.1 (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14))\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.10 in /usr/local/lib/python3.12/dist-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (0.2.14)\n",
            "Requirement already satisfied: rich>=11.0.0 in /usr/local/lib/python3.12/dist-packages (from rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->rich-argparse>=1.7.1->cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->-r requirements.txt (line 14)) (0.1.2)\n",
            "Downloading torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_colorlog-1.2.0-py3-none-any.whl (3.6 kB)\n",
            "Downloading hydra_optuna_sweeper-1.2.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rootutils-1.0.7-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.2/308.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmaes-0.12.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cliff-4.11.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autopage-0.5.2-py3-none-any.whl (30 kB)\n",
            "Downloading cmd2-2.7.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.3/154.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stevedore-5.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: stevedore, rootutils, portalocker, lightning-utilities, colorlog, cmaes, autopage, hydra-core, rich-argparse, hydra-colorlog, torchmetrics, torchdata, cmd2, pytorch-lightning, cliff, optuna, lightning, hydra-optuna-sweeper\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autopage-0.5.2 cliff-4.11.0 cmaes-0.12.0 cmd2-2.7.0 colorlog-6.9.0 hydra-colorlog-1.2.0 hydra-core-1.3.2 hydra-optuna-sweeper-1.2.0 lightning-2.5.5 lightning-utilities-0.15.2 optuna-2.10.1 portalocker-3.2.0 pytorch-lightning-2.5.5 rich-argparse-1.7.1 rootutils-1.0.7 stevedore-5.5.0 torchdata-0.7.1 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "if \"WANDB_API_KEY\" not in os.environ:\n",
        "    os.environ[\"WANDB_API_KEY\"] = getpass.getpass(\"Enter your W&B API key: \")\n",
        "# optional defaults\n",
        "os.environ.setdefault(\"WANDB_ENTITY\", \"hp-learning-rules\")\n",
        "os.environ.setdefault(\"WANDB_PROJECT\", \"decouple-alphas-grid\")"
      ],
      "metadata": {
        "id": "ngKK0vtgL1ce",
        "outputId": "3838aaf8-70af-4ffb-8b33-c0639866f81a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "id": "ngKK0vtgL1ce",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your W&B API key: ··········\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'decouple-alphas-grid'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "from itertools import product\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- paths & config ----\n",
        "REPO_DIR = Path(\"/content/HeterosynapticLearning\").resolve()  # repo root\n",
        "SWEEP_CONFIG = \"grid\"\n",
        "PROJECT = f\"decouple-alphas-{SWEEP_CONFIG}\"\n",
        "USE_DATA_PARAM = True   # pass data.data_dir if data/ exists\n",
        "SLURM = False           # we're on Colab\n",
        "\n",
        "# ---- grid ----\n",
        "grid = {\n",
        "    \"default\": {\n",
        "        \"model\": [\"basic_mlp\"],\n",
        "        \"task\": [\"fmnist\"],\n",
        "        \"optimizer.lr\": [0.1, 0.5],\n",
        "        \"corruption.alpha\": [0.01, 0.25, 0.5, 0.75, 0.99],\n",
        "        \"trainer.min_epochs\": [20],\n",
        "        \"trainer.max_epochs\": [20],\n",
        "    },\n",
        "    \"md\": {\n",
        "        \"optimizer.update_alg\": [\"md\"],\n",
        "        \"optimizer.alpha\": [0.01, 0.25, 0.5, 0.75, 0.99],\n",
        "        \"optimizer.block_size\": [\"4\"],\n",
        "    },\n",
        "    # \"gd\": {\"optimizer.update_alg\": [\"gd\"]},\n",
        "}\n",
        "\n",
        "LOGS_ROOT = REPO_DIR / \"logs\" / PROJECT\n",
        "LOGS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def print_grid_stats(grid):\n",
        "    default = grid.get(\"default\", {})\n",
        "    total = 0\n",
        "    print(\"Grid Search Stats:\\n\")\n",
        "    for space, params in grid.items():\n",
        "        if space == \"default\":\n",
        "            continue\n",
        "        full_params = {**default, **params}\n",
        "        keys = sorted(full_params.keys())\n",
        "        n = 1\n",
        "        for k in keys:\n",
        "            n *= len(full_params[k])\n",
        "        print(f\"  - {space}: {n} configurations\")\n",
        "        total += n\n",
        "    print(f\"\\nTotal configurations: {total}\\n\")\n",
        "\n",
        "def launch_job(**hp):\n",
        "    # deterministic name\n",
        "    name = \"_\".join([str(hp[k]) for k in sorted(hp)])\n",
        "    study_name = f\"study_{name}\"\n",
        "    out_dir = LOGS_ROOT / study_name\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    cmd = [\n",
        "        \"python\", str(REPO_DIR / \"src\" / \"train.py\"), \"-m\",\n",
        "        f\"hydra.sweeper.study_name={study_name}\",\n",
        "        f\"hparams_search={SWEEP_CONFIG}\",\n",
        "        f\"logger.group={name}\",\n",
        "        f\"logger.project={PROJECT}\",\n",
        "        f\"save_dir={out_dir}\",\n",
        "    ]\n",
        "\n",
        "    cmd += [\"trainer.accelerator=gpu\", \"trainer.devices=1\"]\n",
        "\n",
        "    if USE_DATA_PARAM:\n",
        "        data_dir = REPO_DIR / \"data\"\n",
        "        if data_dir.exists():\n",
        "            cmd.append(f\"data.data_dir={data_dir}\")\n",
        "        else:\n",
        "            print(\"⚠️ data/ not found in repo; skipping data.data_dir param.\")\n",
        "\n",
        "    for key, value in hp.items():\n",
        "        cmd.append(f\"{key}={value}\")\n",
        "\n",
        "    # Make sure we run from repo root (Hydra relative paths etc.)\n",
        "    print(\"⏩ Running:\", \" \".join(map(str, cmd)))\n",
        "    subprocess.run(cmd, cwd=str(REPO_DIR), check=True)\n",
        "\n",
        "# --- run ---\n",
        "print_grid_stats(grid)\n",
        "\n",
        "for space, params in grid.items():\n",
        "    if space == \"default\":\n",
        "        continue\n",
        "    full_params = {**grid[\"default\"], **params}\n",
        "    keys = sorted(full_params.keys())\n",
        "    values_list = [full_params[k] for k in keys]\n",
        "    for values in product(*values_list):\n",
        "        hp = {k: v for k, v in zip(keys, values)}\n",
        "        launch_job(**hp)"
      ],
      "metadata": {
        "id": "Dw6axG8GOFAT",
        "outputId": "8d7cd6bf-52fc-4069-9da4-f2d4b23900a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Dw6axG8GOFAT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Search Stats:\n",
            "\n",
            "  - md: 50 configurations\n",
            "\n",
            "Total configurations: 50\n",
            "\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.01_4_0.1_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.01 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.01_4_0.5_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.01 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.25_4_0.1_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.25 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.25_4_0.5_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.25 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.5_4_0.1_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.5 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.5_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.5_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.5_4_0.5_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.5 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.75_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.75_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.75_4_0.1_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.75 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.75_4_0.5_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.75_4_0.5_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.75_4_0.5_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.75 optimizer.block_size=4 optimizer.lr=0.5 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n",
            "⏩ Running: python /content/HeterosynapticLearning/src/train.py -m hydra.sweeper.study_name=study_0.01_basic_mlp_0.99_4_0.1_md_fmnist_20_20 hparams_search=grid logger.group=0.01_basic_mlp_0.99_4_0.1_md_fmnist_20_20 logger.project=decouple-alphas-grid save_dir=/content/HeterosynapticLearning/logs/decouple-alphas-grid/study_0.01_basic_mlp_0.99_4_0.1_md_fmnist_20_20 trainer.accelerator=gpu trainer.devices=1 data.data_dir=/content/HeterosynapticLearning/data corruption.alpha=0.01 model=basic_mlp optimizer.alpha=0.99 optimizer.block_size=4 optimizer.lr=0.1 optimizer.update_alg=md task=fmnist trainer.max_epochs=20 trainer.min_epochs=20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IU1Yy0OkPc-G"
      },
      "id": "IU1Yy0OkPc-G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}